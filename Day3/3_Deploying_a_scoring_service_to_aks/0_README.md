# Deploying a scoring service to the Azure Container Service (AKS)

This hands-on lab guides us through deploying a Machine Learning scoring function to a remote environment using [Azure Machine Learning](https://docs.microsoft.com/en-us/azure/machine-learning/preview/overview-what-is-azure-ml). 

In this lab, we will

- Understand how to create a model file
- Generate a scoring script and schema file
- Prepare the scoring environment
- Deploy the model
- Run and update the real-time web service

***NOTE:*** There are several pre-requisites for this course, including an understanding and implementation of

- Machine Learning and Data Science
- Intermediate to Advanced Python programming
- Familiarity with Docker containers and Kubernetes

There is a comprehensive Learning Path we can use to prepare for this course [located here](https://github.com/Azure/learnAnalytics-CreatingSolutionswiththeTeamDataScienceProcess-/blob/master/Instructions/Learning%20Path%20-%20Creating%20Solutions%20with%20the%20Team%20Data%20Science%20Process.md).

## Building the scoring for remote deployment

The general configuration for working with the Azure Container Service (AKS) has this architecture:

![AKS](https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/15159959-b5cd-4fe9-aeba-441139943ecd.png)

We will review these articles in class:

1. [A quick overview of the Azure Container Service (AKS)](https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough)
2. [Understanding Service Principals](https://docs.microsoft.com/en-us/azure/aks/kubernetes-service-principal)
3. [Scoring Setup and Configuration](https://docs.microsoft.com/en-us/azure/machine-learning/preview/deployment-setup-configuration)
4. [Scaling Clusters](https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-scale-clusters)

### Section 1: Generate files required for scoring

In prior labs, we have created a churn prediction project and run experiments to estimate models that we can use for subsequent scoring. If you have not already, please complete the initial sections of the [prior lab](../lab04.1-managing_models_with_aml/0_README.md) in order to create all of the assets required of a scoring service.

After completing those steps, you should have access to the following in the root directory of the project:

- `score.py` is downloaded as part of the template
- `model.pkl` is generated in the `outputs` section of the run history, and was manually downloaded into the project's root dir
- `service_schema.json` is generated by running `python churn_schema_gen.py`.

In a prior lab, the `model.pkl` file was registered when a local service was created to serve the model's predictions. Registering a model allows us to later do roll-backs if need be.

![CATelcoCustomer](images/CATelcoCustomer_gWithoutDprep.png)

With these files, we can deploy a service to production.

### Section 2: Deploy Service to production

In this section, we will recreate the service, but this time not as a local service but a remote service. To deploy the web service to a production environment, first set up the environment using the following command:

```
az ml env setup --cluster -n <ENVIRONMENT_NAME> -l <AZURE_REGION e.g. eastus2> [-g <RESOURCE_GROUP>]
```

Respond with no to the question about `Reuse storage and ACR (Y/n)?`. This sets up an AKS cluster with Kubernetes as the orchestrator. The cluster environment setup command creates the following resources in our subscription:

1. A resource group (if not provided, or if the name provided does not exist)
2. A storage account (use the existing one)
3. An Azure Container Registry (ACR)
4. A Kubernetes deployment on an Azure Container Service (AKS) cluster
5. An Application insights account

The resource group, storage account, and ACR are created quickly. The AKS deployment can take up to 20 minutes. We use the following command to check the status of an ongoing cluster provisioning:

```
az ml env show -n <ENVIRONMENT_NAME> -g <RESOURCE_GROUP>
```

If the deployment fails the first time and we get an error message saying `Resource quota limit exceeded.` then it means we are over the utilization limit for our subscription. In this case, we can go to the Azure portal and delete any resources we are not using, then delete the above cluster `az ml env delete --cluster <ENVIRONMENT_NAME> -g <RESOURCE_GROUP>`, and finally re-create the cluster using `az ml env setup...` as we did earlier.

Ensure that `"Provisioning State"` changes from `"Creating"` to `"Succeeded"` before proceeding further. Once this is done, we can set the above environment as our compute environment:

```
az ml env set -n <ENVIRONMENT_NAME> -g <RESOURCE_GROUP>
```

A model management account is required for deploying models. We usually do this once per subscription, and can reuse the same account in multiple deployments.

We already have a model management account which was created for us when we provisioned it from the Azure portal along with the Experimentation account. But in this lab we create a new account and this time we use the Azure CLI to do it:

```
az ml account modelmanagement create -l <AZURE_REGION e.g. eastus2> -n <ACCOUNT_NAME> -g <RESOURCE_GROUP> --sku-instances <NUMBER_OF_INSTANCES, e.g. 1> --sku-name <PRICING_TIER for example S1>
```

To use an existing account, use the following command:

```
az ml account modelmanagement set -n <ACCOUNT_NAME> -g <RESOURCE_GROUP>
```

To deploy the saved model as a web service, we execute the below command:

```
az ml service create realtime --model-file <MODEL_FILE_RELATIVE_PATH> -f <SCORING_FILE e.g. score.py> -n <SERVICE_NAME> -s <SCHEMA_FILE e.g. service_schema.json> -r <DOCKER_RUNTIME e.g. spark-py or python> 
```

While the above command provides us with a single step execution it helps to break it down into multiple steps to see what is happening in the background. We can see what the different steps are by just looking at the output generated by the above command. The first step consists of loading the trained model `model.pkl` into the Azure model registry. To look at available models in the model registry we can run the following command:

```
az ml model list -o table
```

Currently, our registered models are not tagged, making it hard to tell them apart. So we begin by re-registering the trained model and properly describing or tagging it.

```
az ml model register -m model.pkl -n naive_bayes -d "Naive Bayes model with default configuration."
```

We can always check the details of registered models using the following commands:

```
az ml model show -m <MODEL_ID>
```

Once a model is created, the next step is to create a manifest from the model.

```
az ml manifest create -n naive_bayes -i <MODEL_ID> -r <RUN_TIME, e.g. python or spark-py> -f <SCORING_SCRIPT, e.g. score.py> -s <SCHEMA_FILE>
```

The above command makes it clear that a model manifest is just a trained model paired with a few dependencies so that it can run as a service. The dependencies are the model's run time, which right now is a choice between `python` and `spark-py`, the python script to execute the scoring, the Conda dependencies to control the python environment, and the schema file to use to check against the in-coming data.

We can check that our manifest was created by running this:

```
az ml manifest show -i <MANIFEST_ID>
```

We have almost all it takes to create a service, but the system dependencies. As we covered this is a prior lab, the best way to handle system dependencies is by using Docker images. These Docker images can then be used to spin off containers that run the service. We can scale the service by spnning off more Docker containers out of the same image. The next command creates a Docker image out of the model manifest we created above.

```
az ml image create -n <IMAGE_NAME> --manifest-id <MANIFEST_ID>
```

Once again we can check our service but simply running the following command:

```
az ml image show -i <IMAGE_ID>
```

Finally, we are now ready to spin a Docker container to host our service.

```
az ml service create realtime -n <SERVICE_NAME> --image-id <IMAGE_ID>
```

That's it. We now have the remote service up and running. We can run some queries against the service using example commands shown in the output generated when we ran the last command. This sequence of commands make clear what the four sequetial steps are that go into creating a service from a model and how these steps tie local resources to the cloud via the Azure Model Management account:

  - register the model
  - create and register a model manifest
  - create and register a Docker image
  - spinn a container from the Docker image

### Section 3: Register a new model

Open and examine `CATelcoCustomerChurnModelingWithoutDprep.py`. In this Python script, we create two models: A naive Bayes model, which is stored in the Python variable called `model` and a decision tree model, stored in the Python variable called `dt`. For the sake of argument, let's assume that the decision tree is a newer and better model (i.e. makes more accurate predictions) than the naive Bayes model. So far we haven't done anything with `dt` once the model is trained. Scroll to them bottom part of the file in order to make a small change: Replace `pickle.dump(model, f)` with `pickle.dump(dt, f)`. This will replace `model.pkl` which contained the earlier Naive Bayes model with the decision tree model represented by `dt`.

Save the changes and run the experiment locally to generate the model file.

```
az ml experiment submit -c local CATelcoCustomerChurnModelingWithoutDprep.py
```

Now based on the commands from the last section, run through the following 

  - register the model under the name `decision_tree` and give a short description
  - create and register a model manifest under the name `decision_tree`
  - create and register a Docker image using the new manifest

Notice that we do not spin a new service from the new image. Instead, in the next seciton, we learn how we can update the current service with the new image. Updating a service has the advantage of letting us keep the service's endpoints unchanged while replacing an older model with a new and improved one. It also allows us to perform roll-backs if things don't work.

### (Optional) Section 4: Update Service with new model

To use a different model in the service, we can perform a simple update to the service. In the Churn Prediction experiment, the accuracy of Decision Tree is slightly higher than Naive Bayes. So, we can update the service to use the decision tree model instead.

There are three steps to perform in order to update the service:

We first register the new model, and we do so under the same name as the old model. This will NOT overwrite the old model. Instead it will create a new version of it, which we can tag using the `-t` and add a description using `-d` arguments.

```
az ml model register -m model.pkl -n decision_tree -d "Using a new model because of higher accuracy"
```

We can see the new model (along with other versions if we had previously registered models under the same name) by running

```
az ml model list -o table
```

We now create a manifest for the model in Azure Container Service. To do so, in the next command, we replace `<MODEL_ID>` with the model ID that was returned in the last command:

```
az ml manifest create -n <MANIFEST_NAME> -f score.py -s service_schema.json -r python -i <MODEL_ID>
```

We now get the manifest ID when we run `az ml manifest create`. Make a note of this id and replace it in the below command when creating image.

```
az ml image create -n <IMAGE_NAME> --manifest-id <MANIFEST_ID>
```

Finally, the last step is to update the existing service out of the new image created. We would need the image ID created from the last step along with the service ID. To obtain the service id, we can run `az ml service list realtime` to get a list of all the service IDs, or we can look up the service on the Azure portal. Run the below command to update the service:

```
az ml service update realtime -i <SERVICE_ID> --image-id <NEW_IMAGE_ID>
```

## Lab Completion

In this lab we learned how to

- Understand how to create a model file
- Generate a scoring script and schema file
- Prepare the scoring environment
- Deploy models to production
- Update a scoring service
